---
title: 关于梯度下降法的理解
date: 2021-07-24 16:07:57
tags: 算法
category: Machine Learning
---
机器学习训练时常用 **梯度下降法** 来进行参数更新

梯度下降法常见有三种：
- 随机梯度下降法（随机使用一个样本进行梯度更新，下图中紫线）
- 小批量梯度下降法（批次样本梯度取平均，下图中绿线）
- 批量梯度下降法（所有样本梯度取平均，下图中蓝线）

<img src="对比图.png" alt='三种梯度下降法对比'>  

> 图中的红色点是最优点，每个箭头所指方向就是当前点的梯度方向

随机梯度下降法是指：
- 随机：随意在当次训练中选取一个样本（训练时会把整个训练集分批「batch_size」进行多次训练「epoch」)
- 梯度：考量这个参数的在该样本方向上的导数
- 下降：缩小这个样本的预测值与实际值之间的差异

随机梯度下降法中的 **随机** 旨在加速，但是可能效果会比使用该批次所有样本进行梯度下降以更新参数差

这里主要说明怎么进行梯度下降
假设在某一层，进行的从 |R<sup>2</sup> 到 |R 的变换
```
Z = a*x + b*y
```
> 其中 a, b 为可训练参数
假设在某一训练阶段得到
```
Z = 3x - y/5
```
此时假设输入为 [1, 5]  => 输出为 2(预测值）
z的实际值为 3

此时采用欧式距离来衡量两者之间的差距

Difference = \sqrt {(z<sub>实际值</sub> - z<sub>预测值</sub>)<sup>2</sup>}

可知 预测值和实际值之间差了1，构建函数

所以我们需要让预测值增大，也就是 最小化差距

因为x,y 是来自于真实数据无法改变，我们需要更改参数大小

比如把a改为4, 就可以使得预测值为3

假设在这一层我们现在仅需要更新 a 参数，把 x 视为给定常数，对a求偏导
```
d(difference)/d(a) = 3
```
也就是沿着这个方向更新参数 a 会使得差距下降最快
a => a<sub>更新前</sub> + learningRate * [d(difference)/d(a)] = a<sub>更新前</sub> + 3*learningRate

根据学习率去更新参数a的大小
> 这里也可以看出学习率必须适当才好，过大过小都会使得参数更新不恰当

当然，机器学习不完全是最优化问题，也就是说大多数的情况下，在本例子中 a 并不能等于 a

因为如果完全变成最优化问题，模型容易过拟合（在本例子中可能针对[1,3]这个输入能够完美拟合，但是对于新的样本效果不够好）







